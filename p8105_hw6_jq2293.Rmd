---
title: "p8105_hw6_jq2293"
author: "JiyueQin"
date: "November 24, 2018"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
theme_set(theme_bw())
knitr::opts_chunk$set(dpi = 200, out.width = "90%")
```

# Problem 1

This problem uses dataset about homicides in 50 large US cities.

## import and tidy

First, import the data from website and do some cleaning.

```{r tidy, message = FALSE}
homicide_data = read_csv("https://github.com/washingtonpost/data-homicides/raw/master/homicide-data.csv") %>% 
  unite(city_state, city, state, sep = ",", remove = FALSE) %>% 
  mutate(solved = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(! city %in% c("Dallas","Phoenix","Kansas City") &
           city_state != "Tulsa,AL") %>% 
  mutate(victim_race = recode_factor(victim_race, White = "white", .default = "non-white"),
         victim_age = as.numeric(victim_age))
```

The initial cleaning steps include:

1. using `unite()` to add a new variable `city_state`.

2. using `mutate()` to add a new variable `solved`.

3. using `filter()` to remove observations in certain cities which lack inforation of victim race.

4. using `mutate()` to modify `victim_race` into categories of white and non-white and convert `victim_age` into numeric variable.

The resulting dataset `homicide_data` has `r nrow(homicide_data)` rows and `r ncol(homicide_data)` columns.

Some key variables are:

- `victim_race`: factor variable, white as reference.

- `city_state`: character varibale, denotes the place of the crime.  

- `lat` and `lon`: numeric variable, denotes the geo location of the crime.

- `solved`: numeric variable(binary), 1 denotes the crime is sloved, meaning closed by arrest. Otherwise, take 0.

## regression for Baltimore

Then, fit a logistic regression to the records in Baltimore. Outcome is resolved vs unresolved and predictors are victim age, sex and race. 

```{r reg}
fit_logit_Bal =
  homicide_data %>% 
  filter(city == "Baltimore") %>% 
  glm(solved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) 

fit_logit_Bal %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         term = str_replace(term, "sex", "sex:"),
         term = str_replace(term, "race", "race:"))%>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
  
result = confint(fit_logit_Bal) %>%
  as.data.frame() %>%
  mutate(term = row.names(.)) %>% 
  full_join(., broom::tidy(fit_logit_Bal), by = "term") %>% 
  select(term, log_OR = estimate, lower_log_OR = "2.5 %", upper_log_OR = "97.5 %") %>% 
  mutate(OR = exp(log_OR),
         lower_OR = exp(lower_log_OR),
         upper_OR = exp(upper_log_OR),
         term = str_replace(term, "sex", "sex:"),
         term = str_replace(term, "race", "race:"))

 knitr::kable(result, digits = 3)
```

From the results of the regression, we can see age, sex and race are all significant predictors at 0.05 level.

For solving homicides, comparing non-white victims to white victims keeping all other variables fixed, the estimated odds ratio is `r round(result[4, 5], 3)`, the 95% confidence is (`r round(result[4, 6], 3)`, `r round(result[4, 7], 3)`). Here, odds ratio is computed using `exp()` to transform original estimate. Smae for the confidence interval.

This indicates non-white victims are more likely to have unsolved homicides than white victims. 

## regression for each city

Run glm for each of the cities and compute estimated ORs and CIs.

```{r reg_all, warning = FALSE, message = FALSE}
summary_reg = function(dataset){
  fit = glm(solved ~ victim_age + victim_sex + victim_race, data = dataset, family = binomial())
  confint(fit) %>%
  as.data.frame() %>%
  mutate(term = row.names(.)) %>% 
  full_join(., broom::tidy(fit), by = "term") %>% 
  select(term, log_OR = estimate, lower_log_OR = "2.5 %", upper_log_OR = "97.5 %") %>% 
  mutate(OR = exp(log_OR),
         lower_OR = exp(lower_log_OR),
         upper_OR = exp(upper_log_OR)) %>% 
  filter(term == "victim_racenon-white")

}

race_estimate = homicide_data %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(result = map(data, summary_reg)) %>% 
  select(-data) %>% 
  unnest() %>% 
  select(-term)
  
 knitr::kable(head(race_estimate), digits = 3)
```

## plot

Finally, a plot is generated to visualize the adjusted odds ratios for non-white victims compared to white victims.

```{r plot}
race_estimate %>% 
  ggplot(aes(x = forcats::fct_reorder(city_state, OR), color = city_state)) +
  geom_point(aes(y = OR)) +
  geom_errorbar(aes(ymin = lower_OR, ymax = upper_OR)) +
  coord_flip() +
  labs(x = "",
       y = "estimated odds ratio",
       title = "adjusted ORs of solving homicides in non-white victims vs whites in 50 cities",
       caption = "error bar showing 95% confidence interval") + 
  theme_bw(base_size = 8) +
  theme(legend.position = "None")
```

From the plot, we can see Boston has the lowest odds ratio(less than 0.25), meaning in Boston, non-white victims are least likely to have solved homicides compared to white victims.  Tampa, Birmingham and Durham have Top3 highest odds ratio, which are a bit higher than 1, meaning the probability of getting a solved homicide is similar between non-whites and whites.

Most cities have estimated odds ratio less than 1.0, showing the trend that non-white victims are less likely to have solved homicides compared to white victims.

About half of the cities have confidence intervals smaller than 1, which indicates in these cities, race is a significant predictor of solved/unsolved homicide at 0.05 level.

# Problem 2

This problem uses dataset about children's birthweight.

## import and tidy

First, import the data and clean for modeling.

```{r prep}
birth_data = read_csv("data/birthweight.csv") %>% 
  mutate(babysex = recode_factor(babysex, `1` = "male", `2` = "female"),
         frace = recode_factor(frace, `1`= "White", `2` = "Black", `3` = "Asian",
                               `4` = "Puerto Rican", `8` = "Other"),
         malform = recode_factor(malform, `0` = "absent",`1` = "present"),
         mrace = recode_factor(mrace, `1`= "White", `2` = "Black", `3` = "Asian", `4` = "Puerto Rican")) %>% 
  select(-pnumlbw, -pnumsga)
  

skimr::skim(birth_data) %>% filter(stat == "missing")
```

This dataset `birth_data` has `r nrow(birth_data)` rows and `r ncol(birth_data)` columns.

After using `skimr::skim()`, no missing value is found. Also, all the values of `pnumlbw` and `pnumsga` are 0, which makes no sense for regression and are removed. Using `recode_factor()` to transform `babysex`, `frace`, `malform` and `mrace` into factors.

## regression

Here, multiple linear regression is adopted. Note, the distribution of birthweight is approximatly normal, which satisfies the assumption for statistical inference.

```{r mlr}
bwt_mlr_1 = lm(bwt ~ gaweeks + malform + smoken + wtgain, data = birth_data)
broom::tidy(bwt_mlr_1)
broom::glance(bwt_mlr_1)

bwt_mlr_2 = lm(bwt ~ gaweeks + smoken + wtgain + bhead + blength + mrace , data = birth_data)
broom::tidy(bwt_mlr_2)
broom::glance(bwt_mlr_2)

birth_data %>% 
  add_residuals(bwt_mlr_2, var = "residual") %>% 
  add_predictions(bwt_mlr_2) %>% 
  ggplot(aes(x = pred, y = residual)) + 
  geom_point(alpha = .3) +
  geom_smooth(se = FALSE) +
  labs(x = "fitted values",
       title = "distribution of model residuals against fitted values") 

```

At first, It is hypothesied that `gaweeks`, `malform`, `smoken` and `wtgain` are predictors, but it turns out `malform` is not a significant predictor, and the R^2 is only 0.21, meaning the variance is accounted for other variables.

Trying `gaweeks`, `smoken`, `wtgain`, `bhead`, `blength`, `mrace` as predictors, we can see each predictor is significant and adjusted R^2 is 0.71, pretty similar to crude R^2, meaning the set of predictors here is reasonable. So the second model is used here.

In general, the plot shows that residuals are randomly scattered around 0, but some outliers can be noticed, and it has high residuals in the area of low fitted values. 

## cross validation

Make comparison in terms of the cross-validated prediction error with another two models.

- `model_1`: length at birth and gestational age as predictors.

- `model_2`: head circumference, length, sex, and all interactions between these as predictors.

```{r validation}

set.seed(1)

cv_df = crossv_mc(birth_data, 100) %>% 
  mutate(model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model_2 = map(train, ~lm(bwt ~ bhead + babysex + blength + bhead * babysex * blength, data = .x)),
         my_model = map(train, ~lm(bwt ~ gaweeks + smoken + wtgain + bhead + blength + mrace , data = .x))) %>% 
  mutate(rmse_model_1 = map2(model_1, test, ~rmse(model = .x, data = .y)),
         rmse_model_2 = map2(model_2, test, ~rmse(model = .x, data = .y)),
         rmse_my_model = map2(my_model, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  unnest() %>% 
  mutate(model = str_replace(model, "rmse_", "")) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin() +
  labs(title = "the distribution of rmse in different models") +
  theme(legend.position = "None") 

```

From the plot, we can see my model has lowest distribution in rmse, while model_1 has the highest distribution of rmse. In general, from the perspective of distribution of rmse, I will keep my original model. But considering the interaction model's performance, it might be helpful to add interaction term in my model.